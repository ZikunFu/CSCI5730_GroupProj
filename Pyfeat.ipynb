{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Elicited and Acted Emotional Expressions in PEDFE\n",
    "Group Member:\n",
    "- Zikun Fu\n",
    "- Tony Wang\n",
    "\n",
    "## Objective\n",
    "The goal of this study is to investigate the differences between elicited and acted emotional expressions within the The Padova Emotional Dataset of Facial Expressions(PEDFE) dataset.\n",
    "\n",
    "## Research Question\n",
    "How do automated emotion recognition systems perform in differentiating between elicited and acted emotional expressions in the PEDFE dataset?\n",
    "\n",
    "## Hypothesis\n",
    "We hypothesize that there will be significant differences in classification accuracy between elicited and acted emotional expressions, reflecting the inherent complexity and subtlety of genuine emotional states.\n",
    "\n",
    "## Methodology\n",
    "1. **Data Preparation**: Utilize the PEDFE dataset for the analysis.\n",
    "2. **Feature Extraction**: Use facial expression analysis tools to extract relevant features from the dataset.\n",
    "3. **Emotion Classification**: Apply classification models to identify patterns and distinctions between elicited and acted emotions.\n",
    "4. **Result Analysis**: Evaluate the classifier's performance and analyze the distinctions between genuine and posed expressions.\n",
    "\n",
    "## Expected Outcomes\n",
    "The study aims to provide insights into the nuances of emotional expression and the ability of current technological systems to recognize genuine emotional states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padova Emotional Dataset for FER\n",
    "\n",
    "[Padova Emotional Dataset Article](https://link.springer.com/article/10.3758/s13428-022-01914-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: \n",
    "\n",
    "To offer a rich dataset of both genuine (N = 707) and posed (N = 751) emotional expressions for the six universal emotions, using 56 participants.\n",
    "\n",
    "- **Versions**: \n",
    "Available in original clips (with participants' body and background) and modified clips (focusing solely on the face).\n",
    "- **Validation**: \n",
    "Original dataset validated by 122 human raters and the modified dataset by 280 human raters. Includes hit rates for emotion and genuineness, mean standard deviation of genuineness, and intensity perception for each clip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "- **Participants**: 57 participants aged 20 to 30, with one withdrawing consent, resulting in contributions from 56 participants.\n",
    "- **Experimental Setup**: Designed to record spontaneous emotions with minimal participant awareness of being filmed to preserve natural expressions.\n",
    "- **Emotion Elicitation Procedure**: Utilized a multi-modal protocol with videos, games, and other tasks to elicit a range of emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental Materials\n",
    "\n",
    "### T1\n",
    "- **Identification Number**: Each subject is assigned a unique identifier.\n",
    "- **Gender**: Marked as F (female) or M (male).\n",
    "- **PEDFE Code**:\n",
    "  - The first character indicates the subject's number.\n",
    "  - The second character represents the emotion (d=disgust, h=happiness, f=fear, a=anger, s=surprise, t=sadness).\n",
    "  - The third character denotes whether the emotion is genuine (g) or posed (s).\n",
    "- **Hit Rate Accuracy Scores**: Percentage of raters correctly recognizing the emotion and its authenticity.\n",
    "- **Intensity**: Rated on scales from 0-9.\n",
    "- **Genuineness**: Rated on scales from -7 to +7.\n",
    "- **Duration**: Length of each clip in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Supplemental Material 1 (T1)\n",
    "t1_desc_path = 'data/Supplemental_Material_T1.csv'\n",
    "t1 = pd.read_csv(t1_desc_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(t1.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2\n",
    "- **Page 1: Emotion Hit Rate**\n",
    "  - Details the average and standard deviation of hit rates for each actor in PEDFE, divided by emotion. This data provides insights into the accuracy of emotion recognition across the dataset.\n",
    "\n",
    "- **Page 2: Genuineness Hit Rate**\n",
    "   - Presents genuineness hit rates for each actor by emotion, including averages and standard deviations. This section highlights how well raters could distinguish between genuine and posed emotions.\n",
    "\n",
    "### T3\n",
    "- **Page 1: Emotion Experience**\n",
    "   - Summarizes the emotions participants reported experiencing for each task, offering a direct view into the emotional impact of the elicitation protocol.\n",
    "\n",
    "- **Page 2: Levels of Genuineness**\n",
    "   - Features ratings from participants on the genuineness of their emotions for each task, ranging from completely not genuine to completely genuine. This self-assessment provides a unique perspective on the authenticity of the expressed emotions.\n",
    "\n",
    "- **Page 3: Intensity Ratings**\n",
    "   - Contains participants' self-rated intensity for each task, on a scale from 0 (none) to 9 (strong). These ratings offer a quantifiable measure of the emotional strength participants felt during each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Py-Feat Toolbox\n",
    "[Pyfeat Documentation](https://py-feat.org/pages/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "The Py-Feat (Python Facial Expression Analysis Toolbox) is a comprehensive library designed for facial feature extraction, emotion recognition, and more, using advanced image processing and machine learning techniques. This toolbox provides an accessible interface to extract and analyze facial expressions from images and videos, enabling researchers to conduct in-depth studies of emotional expressions.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Facial Landmark Detection**: Py-Feat utilizes state-of-the-art models like `retinaface` to detect facial landmarks accurately, which are crucial for analyzing facial expressions.\n",
    "- **Action Unit (AU) Recognition**: The toolbox can detect facial Action Units, which are specific movements of facial muscles correlating to various emotions, using models like `xgb` (XGBoost).\n",
    "- **Emotion Classification**: It employs pre-trained models like `resmasknet` to classify facial expressions into emotion categories such as happiness, sadness, anger, etc.\n",
    "- **Pose Estimation**: With the `img2pose` model, Py-Feat estimates the orientation of the face in the image, providing context to the emotional expression analysis.\n",
    "- **Performance Enhancement**: Py-Feat is designed to leverage GPU acceleration (`device='cuda'`), significantly speeding up the processing of large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Detector\n",
    "- Class that is a combination of a Face, Landmark, Action Unit, and Emotion detection model into a single object<br>\n",
    "- Available models: https://py-feat.org/pages/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat import Detector\n",
    "\n",
    "detector = Detector(\n",
    "    face_model=\"retinaface\",\n",
    "    landmark_model=\"mobilefacenet\",\n",
    "    au_model='xgb',\n",
    "    emotion_model=\"resmasknet\",\n",
    "    facepose_model=\"img2pose\",\n",
    "    device='cuda' #Use GPU Acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector\n",
    "- `face_model=\"retinaface\"`: Specifies the use of the RetinaFace model for face detection. RetinaFace is known for its accuracy in detecting faces across various orientations and scales.\n",
    "\n",
    "- `landmark_model=\"mobilefacenet\"`: Chooses MobileFaceNet as the model for detecting facial landmarks. MobileFaceNet is efficient and designed for mobile and embedded vision applications, balancing accuracy and computational resource use.\n",
    "\n",
    "- `au_model='xgb'`: Utilizes an XGBoost (Extreme Gradient Boosting) model for recognizing facial action units. Action units are fundamental components of facial expressions, corresponding to contractions of specific facial muscles.\n",
    "\n",
    "- `emotion_model=\"resmasknet\"`: Employs the ResMaskNet model for emotion detection. This model is capable of classifying various emotional states by analyzing facial features and expressions.\n",
    "\n",
    "- `facepose_model=\"img2pose\"`: Uses the img2pose model to estimate the pose of the face. This includes the orientation of the face in terms of pitch, yaw, and roll angles, providing context for the facial expressions in relation to the camera perspective.\n",
    "\n",
    "- `device='cuda'`: Specifies that the GPU should be used for computation, leveraging CUDA for acceleration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Videos\n",
    " - https://py-feat.org/basic_tutorials/03_detector_vids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Directory containing videos\n",
    "video_directory = 'data/PEDFE_set_clips'\n",
    "\n",
    "# List all video files in the directory\n",
    "video_files = [f for f in os.listdir(video_directory) if f.endswith(('.avi'))]\n",
    "\n",
    "# Process each video\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(video_directory, video_file)\n",
    "    try:\n",
    "        # Process the video, skipping frames for efficiency, and aggregate the results\n",
    "        results = detector.detect_video(video_path, skip_frames=24, aggregate=True)\n",
    "        \n",
    "        if not results.empty:\n",
    "            # Since results are aggregated, we get a single row of results summarizing the video\n",
    "            aggregated_results = results.iloc[0]  # Access the aggregated results\n",
    "            print(f\"Aggregated results for {video_file}:\")\n",
    "            # display how many predictions have been produced\n",
    "            #print(results.shape)\n",
    "            print(aggregated_results)\n",
    "\n",
    "        else:\n",
    "            print(f\"No faces detected in {video_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {video_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_prediction.loc[[24, 72]].plot_detections(faceboxes=False, add_titles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = video_prediction.emotions.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyfeat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
